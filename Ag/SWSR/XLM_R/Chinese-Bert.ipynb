{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from MyKu import MyBERT\n",
    "from MyKu import processing\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 训练准备阶段，设置超参数和全局变量\n",
    "file_name = 'readme.md'\n",
    "batch_size = 8\n",
    "num_epoch = 10  # 训练轮次\n",
    "check_step = 1  # 用以训练中途对模型进行检验：每check_step个epoch进行一次测试和保存模型\n",
    "\n",
    "learning_rate = 1e-5  # 优化器的学习率\n",
    "\n",
    "# 获取训练、测试数据、分类类别总数\n",
    "data = processing.load_swsr()\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "categories = 2\n",
    "\n",
    "train_iter, test_iter = MyBERT.load_bert_data(\n",
    "    train_data, test_data, batch_size)\n",
    "\n",
    "#固定写法，可以牢记，cuda代表Gpu\n",
    "# torch.cuda.is_available()可以查看当前Gpu是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练模型，因为这里是英文数据集，需要用在英文上的预训练模型：bert-base-uncased\n",
    "# uncased指该预训练模型对应的词表不区分字母的大小写\n",
    "\n",
    "\n",
    "# 详情可了解：https://huggingface.co/bert-base-uncased\n",
    "pretrained_model_name = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "# 创建模型 BertSST2Model\n",
    "model = MyBERT.MyBertModel(categories, pretrained_model_name)\n",
    "# 固定写法，将模型加载到device上，\n",
    "# 如果是GPU上运行，此时可以观察到GPU的显存增加\n",
    "model.to(device)\n",
    "\n",
    "# 训练过程\n",
    "# Adam是最近较为常用的优化器，详情可查看：https://www.jianshu.com/p/aebcaf8af76e\n",
    "optimizer = Adam(model.parameters(), learning_rate)  # 使用Adam优化器\n",
    "loss = nn.CrossEntropyLoss()  # 使用crossentropy作为二分类任务的损失函数\n",
    "\n",
    "# 记录当前训练时间，用以记录日志和存储\n",
    "timestamp = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = 'chinese-bert.md'\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    MyBERT.train(model, train_iter, device, optimizer, loss, epoch)\n",
    "    # MyXLM_Base.train(model, de_train_iter, device, optimizer, loss, epoch)\n",
    "    # MyXLM_Base.train(model, hi_train_iter, device, optimizer, loss, epoch)\n",
    "    en_acc_score = MyBERT.test(\n",
    "        model, test_iter, device, epoch, file_name)\n",
    "    # de_acc_score = MyXLM_Base.test(model, de_test_iter, device, epoch, file_name)\n",
    "    # hi_acc_score = MyXLM_Base.test(model, hi_test_iter, device, epoch, file_name)\n",
    "    print('\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext-large')\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence = ['在联合任务的训练下，我们的模型取得了最好的实验效果！']\n",
    "inputs = tokenizer(sentence, return_tensors='pt',\n",
    "                   padding=True, truncation=True)\n",
    "\n",
    "print(inputs)\n",
    "inputs_ids = inputs['input_ids']\n",
    "# inputs_ids = inputs_ids.squeeze(0)\n",
    "\n",
    "words = tokenizer.convert_ids_to_tokens(inputs_ids[0])\n",
    "print(words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='hfl/chinese-roberta-wwm-ext-large', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext-large')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1762, 5468, 1394,  818, 1218, 4638, 6378, 5298,  678, 8024, 2769,\n",
      "          812, 4638, 3563, 1798, 1357, 2533,  749, 3297, 1962, 4638, 2141, 7741,\n",
      "         3126, 3362, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n",
      "['[CLS]', '在', '联', '合', '任', '务', '的', '训', '练', '下', '，', '我', '们', '的', '模', '型', '取', '得', '了', '最', '好', '的', '实', '验', '效', '果', '！', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = ['在联合任务的训练下，我们的模型取得了最好的实验效果！']\n",
    "inputs = tokenizer(sentence, return_tensors='pt',\n",
    "                   padding=True, truncation=True)\n",
    "\n",
    "print(inputs)\n",
    "inputs_ids = inputs['input_ids']\n",
    "# inputs_ids = inputs_ids.squeeze(0)\n",
    "\n",
    "words = tokenizer.convert_ids_to_tokens(inputs_ids[0])\n",
    "print(words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}