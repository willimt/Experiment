{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.command.config import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging\n",
    "import processing\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from MyKu import processing\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processing.get_OLID_train_data()\n",
    "test_data = processing.get_OLID_testA_data()\n",
    "\n",
    "MAX_LEN = 60\n",
    "\n",
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name, do_lower_case=True)\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for sent, label in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        labels.append(label)\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "    attention_masks = torch.Tensor(attention_masks)\n",
    "    # print(attention_masks)\n",
    "    labels = torch.Tensor(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_inputs, train_masks, train_labels = preprocessing_for_bert(train_data)\n",
    "test_inputs, test_masks, test_labels = preprocessing_for_bert(test_data)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "test_iter = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epoch=10):\n",
    "    bert_model = MyBertModel(MAX_LEN, 2)\n",
    "    bert_model.to(DEVICE)\n",
    "    learning_rate = 2e-5\n",
    "    optimizer = AdamW(\n",
    "        bert_model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    total_steps = len(train_iter) * epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    return bert_model, optimizer, scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "def train(model, train_iter, test_iter, optimizer, scheduler, epochs=10, evaluation=None):\n",
    "    for num_epoch in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'每40个Batch':^9} | {'训练集 Loss':^12} | {'测试集 Loss':^10} | {'测试集准确率':^9} | {'时间':^9}\")\n",
    "        print(\"-\" * 80)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_att_masks, b_labels = tuple(\n",
    "                t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "            model.zero_grad()\n",
    "            # b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            output = model.classifer(model(b_input_ids, b_att_masks))\n",
    "            # print(output.dtype)\n",
    "            b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            loss = loss_fn(output, b_labels.long())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if(step % 40 == 0 and step != 0) or (step == len(train_iter) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(\n",
    "                    f\"{num_epoch + 1:^7} | {step:^10}  | {batch_loss / batch_counts:^14.6f}  | {'-':^12} | {'-':^13} |  {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_iter)\n",
    "        print('-' * 80)\n",
    "\n",
    "        if evaluation:\n",
    "            test_loss, test_accuracy, f1_score = evaluate(model, test_iter)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{num_epoch + 1:^7} | {'-':^10} | {avg_train_loss:^14.6f} | {test_loss:^12.6f} | {test_accuracy:^12.2f} | {f1_score:^12.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    global best_score\n",
    "    \"\"\"\n",
    "    在每个epoch后验证集上评估model性能\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 准确率和误差\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    pred_y, true_y = [], []\n",
    "    # 验证集上的每个batch\n",
    "    for batch in test_dataloader:\n",
    "        # 放到GPU上\n",
    "        b_input_ids, b_attn_masks, b_labels = tuple(\n",
    "            t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "        # 计算结果，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            # 放到model里面去跑，返回验证集的ouput就是一行三列的\n",
    "            output = model.classifer(model(b_input_ids, b_attn_masks))\n",
    "        # 计算误差\n",
    "        loss = loss_fn(output, b_labels.long())\n",
    "        test_loss.append(loss.item())\n",
    "        pred = torch.argmax(output, dim=1).flatten()\n",
    "        # get预测结果，这里就是求每行最大的索引咯，然后用flatten打平成一维\n",
    "        # 计算准确率，这个就是俩比较，返回相同的个数, .cpu().numpy()就是把tensor从显卡上取出来然后转化为numpy类型的举证好用方法\n",
    "        # 最后mean因为直接bool形了，也就是如果预测和label一样那就返回1，正好是正确的个数，求平均就是准确率了\n",
    "        accuracy = (pred == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "        pred_y.extend(pred.tolist())\n",
    "        true_y.extend(b_labels.tolist())\n",
    "    # print(len(true_y))\n",
    "    # 计算整体的平均正确率和loss\n",
    "    print(metrics.confusion_matrix(true_y, pred_y))\n",
    "    val_loss = np.mean(test_loss)\n",
    "    val_accuracy = np.mean(test_accuracy)\n",
    "    f1_socre = metrics.f1_score(true_y, pred_y, average=\"macro\") * 100\n",
    "    if f1_socre > 80 and f1_socre > best_score:\n",
    "        best_score = f1_socre\n",
    "        path = 'D:/Experiment_models_save/OLID/'\n",
    "        save_pretrained(model, path)\n",
    "\n",
    "    return val_loss, val_accuracy, f1_socre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertModel(nn.Module):\n",
    "\n",
    "    def __init__(self, len, class_size, pretrained_name='bert-base-uncased'):\n",
    "        super(MyBertModel, self).__init__()\n",
    "        config = BertConfig.from_pretrained(pretrained_name)\n",
    "        config.output_hidden_states = True\n",
    "        # config.output_attentions = True\n",
    "        config.return_dict = True\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name, config=config)\n",
    "        self.w_h = nn.Parameter(torch.Tensor(len))\n",
    "        self.b_n = nn.Parameter(torch.Tensor(768))\n",
    "        self.W_h = nn.Parameter(torch.Tensor(768, 768))\n",
    "        self.W_e = nn.Parameter(torch.Tensor(768, 768))\n",
    "        self.W_hs = nn.Parameter(torch.Tensor(768, 768))\n",
    "        self.W_L1 = nn.Parameter(torch.Tensor(768 * 3, 768))\n",
    "        self.b_L1 = nn.Parameter(torch.Tensor(len, 768))\n",
    "        self.W_L2 = nn.Parameter(torch.Tensor(768, class_size))\n",
    "        self.b_L2 = nn.Parameter(torch.Tensor(len, class_size))\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.len = len\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.classifer = nn.Linear(len, class_size)\n",
    "        nn.init.uniform_(self.w_h, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_h, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_e, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.b_n, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_hs, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_L1, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.b_L1, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_L2, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.b_L2, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0] # Last_hidden size [32, 80, 768]\n",
    "        Embedding = outputs[2][0] # Embedding size [32, 80, 768]\n",
    "        hs = self.tanh(torch.matmul(Embedding.permute(0, 2, 1), self.w_h) + self.b_n) # hs size [32, 768]\n",
    "        hs = torch.repeat_interleave(torch.unsqueeze(hs, 1), self.len, 1) #hs size [32, 80, 768]\n",
    "        h_f = torch.cat((torch.cat((torch.matmul(last_hidden_state_cls, self.W_h), torch.matmul(Embedding, self.W_e)), dim=-1), torch.matmul(hs, self.W_hs)), dim=-1)\n",
    "        # hf size [32, 80, 768 * 3]\n",
    "        L1 = self.relu(torch.matmul(h_f, self.W_L1) + self.b_L1) # L1 size [32, 80, 768]\n",
    "        L2 = self.softmax(torch.matmul(L1, self.W_L2) + self.b_L2) # L2 size [32, 80, 2]\n",
    "        outs = L2[:, :, 1]\n",
    "        return outs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     40      |    0.653406     |      -       |       -       |    15.20  \n",
      "   1    |     80      |    0.638056     |      -       |       -       |    13.74  \n",
      "   1    |    120      |    0.588376     |      -       |       -       |    13.78  \n",
      "   1    |    160      |    0.535470     |      -       |       -       |    13.85  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Experiment\\Ag\\Offensive words detection model\\model.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bert_model, optimizer, scheduler \u001b[39m=\u001b[39m initialize_model(\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart training and testing:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(bert_model, train_iter,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       test_iter, optimizer, scheduler, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, evaluation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32md:\\Experiment\\Ag\\Offensive words detection model\\model.ipynb Cell 6\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, test_iter, optimizer, scheduler, epochs, evaluation)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), \u001b[39m1.0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:55\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[0;32m     53\u001b[0m clip_coef_clamped \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(clip_coef, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters:\n\u001b[1;32m---> 55\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mmul_(clip_coef_clamped\u001b[39m.\u001b[39;49mto(p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdevice))\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m total_norm\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "      bert_model, optimizer, scheduler = initialize_model(2)\n",
    "      print(\"Start training and testing:\\n\")\n",
    "      train(bert_model, train_iter,\n",
    "            test_iter, optimizer, scheduler, epochs=3, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     40      |    0.653406     |      -       |       -       |    15.20  \n",
      "   1    |     80      |    0.638056     |      -       |       -       |    13.74  \n",
      "   1    |    120      |    0.588376     |      -       |       -       |    13.78  \n",
      "   1    |    160      |    0.535470     |      -       |       -       |    13.85  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32md:\\Experiment\\Ag\\Offensive words detection model\\model.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bert_model, optimizer, scheduler \u001b[39m=\u001b[39m initialize_model(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart training and testing:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(bert_model, train_iter,\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       test_iter, optimizer, scheduler, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, evaluation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1;32md:\\Experiment\\Ag\\Offensive words detection model\\model.ipynb Cell 6\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, test_iter, optimizer, scheduler, epochs, evaluation)\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), \u001b[39m1.0\u001b[39;49m)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/Offensive%20words%20detection%20model/model.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:55\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n",
      "\u001b[0;32m     53\u001b[0m clip_coef_clamped \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(clip_coef, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters:\n",
      "\u001b[1;32m---> 55\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mmul_(clip_coef_clamped\u001b[39m.\u001b[39;49mto(p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdevice))\n",
      "\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m total_norm\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "      bert_model, optimizer, scheduler = initialize_model(2)\n",
    "      print(\"Start training and testing:\\n\")\n",
    "      train(bert_model, train_iter,\n",
    "            test_iter, optimizer, scheduler, epochs=3, evaluation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
