{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.command.config import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging\n",
    "import processing\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from MyKu import processing\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.create_HateXplain_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = processing.get_HateXplain_train_data()\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "\n",
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for sent, label in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        labels.append(label)\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "    attention_masks = torch.Tensor(attention_masks)\n",
    "    # print(attention_masks)\n",
    "    labels = torch.Tensor(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_inputs, train_masks, train_labels = preprocessing_for_bert(train_data)\n",
    "test_inputs, test_masks, test_labels = preprocessing_for_bert(test_data)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "test_iter = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertModel(nn.Module):\n",
    "\n",
    "    def __init__(self, class_size, pretrained_name='bert-base-uncased'):\n",
    "\n",
    "        # 类继承的初始化，固定写法\n",
    "        super(MyBertModel, self).__init__()\n",
    "        # 加载HuggingFace的BertModel\n",
    "        # BertModel的最终输出维度默认为768\n",
    "        # return_dict=True 可以使BertModel的输出具有dict属性，即以 bert_output['last_hidden_state'] 方式调用\n",
    "\n",
    "        config = BertConfig.from_pretrained(pretrained_name)\n",
    "        config.output_attentions = True\n",
    "        config.return_dict = True\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name, config=config)\n",
    "        # 通过一个线性层将[CLS]标签对应的维度：768->class_size\n",
    "        # class_size 在SST-2情感分类任务中设置为：2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, class_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取DataLoader中已经处理好的输入数据：\n",
    "        # input_ids :tensor类型，shape=batch_size*max_len   max_len为当前batch中的最大句长\n",
    "        # input_tyi :tensor类型，\n",
    "        # input_attn_mask :tensor类型，因为input_ids中存在大量[Pad]填充，attention mask将pad部分值置为0，让模型只关注非pad部分\n",
    "\n",
    "        # 将三者输入进模型，如果想知道模型内部如何运作，前面的蛆以后再来探索吧~\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # bert_output 分为两个部分：\n",
    "        #   last_hidden_state:最后一个隐层的值\n",
    "        #   pooler output:对应的是[CLS]的输出,用于分类任务\n",
    "        # 通过线性层将维度：768->2\n",
    "        # categories_numberic：tensor类型，shape=batch_size*class_size，用于后续的CrossEntropy计算\n",
    "        categories_numberic = self.classifier(last_hidden_state_cls)\n",
    "        return categories_numberic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epoch=3):\n",
    "    bert_model = MyBertModel(2)\n",
    "    bert_model.to(DEVICE)\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = AdamW(\n",
    "        bert_model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    total_steps = len(train_iter) * epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    return bert_model, optimizer, scheduler\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, test_iter, optimizer, scheduler, epochs=10, evaluation=None):\n",
    "    for num_epoch in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'每40个Batch':^9} | {'训练集 Loss':^12} | {'测试集 Loss':^10} | {'测试集准确率':^9} | {'时间':^9}\")\n",
    "        print(\"-\" * 80)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_att_masks, b_labels = tuple(\n",
    "                t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "            model.zero_grad()\n",
    "            # b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            output = model(b_input_ids, b_att_masks)\n",
    "            # print(output.dtype)\n",
    "            b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            loss = loss_fn(output, b_labels.long())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if(step % 40 == 0 and step != 0) or (step == len(train_iter) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(\n",
    "                    f\"{num_epoch + 1:^7} | {step:^10}  | {batch_loss / batch_counts:^14.6f}  | {'-':^12} | {'-':^13} |  {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_iter)\n",
    "        print('-' * 80)\n",
    "\n",
    "        if evaluation:\n",
    "            test_loss, test_accuracy, f1_score = evaluate(model, test_iter)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{num_epoch + 1:^7} | {'-':^10} | {avg_train_loss:^14.6f} | {test_loss:^12.6f} | {test_accuracy:^12.2f} | {f1_score:^12.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    在每个epoch后验证集上评估model性能\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 准确率和误差\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    pred_y, true_y = [], []\n",
    "    # 验证集上的每个batch\n",
    "    for batch in test_dataloader:\n",
    "        # 放到GPU上\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(\n",
    "            t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "        # 计算结果，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            # 放到model里面去跑，返回验证集的ouput就是一行三列的\n",
    "            output = model(b_input_ids, b_attn_mask)\n",
    "        # 计算误差\n",
    "        loss = loss_fn(output, b_labels.long())\n",
    "        test_loss.append(loss.item())\n",
    "        pred = torch.argmax(output, dim=1).flatten()\n",
    "        # get预测结果，这里就是求每行最大的索引咯，然后用flatten打平成一维\n",
    "        # 计算准确率，这个就是俩比较，返回相同的个数, .cpu().numpy()就是把tensor从显卡上取出来然后转化为numpy类型的举证好用方法\n",
    "        # 最后mean因为直接bool形了，也就是如果预测和label一样那就返回1，正好是正确的个数，求平均就是准确率了\n",
    "        accuracy = (pred == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "        pred_y.extend(pred.tolist())\n",
    "        true_y.extend(b_labels.tolist())\n",
    "    # print(len(true_y))\n",
    "    # 计算整体的平均正确率和loss\n",
    "    print(metrics.confusion_matrix(true_y, pred_y))\n",
    "    val_loss = np.mean(test_loss)\n",
    "    val_accuracy = np.mean(test_accuracy)\n",
    "    f1_socre = metrics.f1_score(true_y, pred_y, average=\"macro\") * 100\n",
    "\n",
    "    return val_loss, val_accuracy, f1_socre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model, optimizer, scheduler = initialize_model(2)\n",
    "print(\"Start training and testing:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     40      |    0.673658     |      -       |       -       |    14.73  \n",
      "   1    |     80      |    0.658289     |      -       |       -       |    13.34  \n",
      "   1    |    120      |    0.639197     |      -       |       -       |    13.34  \n",
      "   1    |    160      |    0.573379     |      -       |       -       |    13.36  \n",
      "   1    |    200      |    0.604560     |      -       |       -       |    13.44  \n",
      "   1    |    240      |    0.577871     |      -       |       -       |    13.41  \n",
      "   1    |    280      |    0.568165     |      -       |       -       |    13.45  \n",
      "   1    |    320      |    0.592539     |      -       |       -       |    13.42  \n",
      "   1    |    360      |    0.549414     |      -       |       -       |    13.42  \n",
      "   1    |    400      |    0.550093     |      -       |       -       |    13.48  \n",
      "   1    |    440      |    0.557803     |      -       |       -       |    13.44  \n",
      "   1    |    480      |    0.566219     |      -       |       -       |    13.43  \n",
      "   1    |    503      |    0.541839     |      -       |       -       |    7.63   \n",
      "--------------------------------------------------------------------------------\n",
      "[[ 863  862]\n",
      " [ 352 1953]]\n",
      "   1    |     -      |    0.590443    |   0.570405   |    69.88     |    67.50     |  180.81  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |     40      |    0.524304     |      -       |       -       |    13.83  \n",
      "   2    |     80      |    0.543472     |      -       |       -       |    13.50  \n",
      "   2    |    120      |    0.534311     |      -       |       -       |    13.55  \n",
      "   2    |    160      |    0.523461     |      -       |       -       |    13.51  \n",
      "   2    |    200      |    0.553065     |      -       |       -       |    13.54  \n",
      "   2    |    240      |    0.523340     |      -       |       -       |    13.57  \n",
      "   2    |    280      |    0.546362     |      -       |       -       |    13.47  \n",
      "   2    |    320      |    0.515913     |      -       |       -       |    13.51  \n",
      "   2    |    360      |    0.535399     |      -       |       -       |    13.49  \n",
      "   2    |    400      |    0.529147     |      -       |       -       |    13.51  \n",
      "   2    |    440      |    0.534613     |      -       |       -       |    13.46  \n",
      "   2    |    480      |    0.516657     |      -       |       -       |    13.50  \n",
      "   2    |    503      |    0.518992     |      -       |       -       |    7.71   \n",
      "--------------------------------------------------------------------------------\n",
      "[[1056  669]\n",
      " [ 508 1797]]\n",
      "   2    |     -      |    0.531077    |   0.566140   |    70.80     |    69.77     |  181.09  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   3    |     40      |    0.495152     |      -       |       -       |    13.82  \n",
      "   3    |     80      |    0.494690     |      -       |       -       |    13.37  \n",
      "   3    |    120      |    0.532741     |      -       |       -       |    13.11  \n",
      "   3    |    160      |    0.472882     |      -       |       -       |    13.12  \n",
      "   3    |    200      |    0.517425     |      -       |       -       |    13.14  \n",
      "   3    |    240      |    0.511726     |      -       |       -       |    13.12  \n",
      "   3    |    280      |    0.515114     |      -       |       -       |    13.12  \n",
      "   3    |    320      |    0.524096     |      -       |       -       |    13.33  \n",
      "   3    |    360      |    0.513543     |      -       |       -       |    13.42  \n",
      "   3    |    400      |    0.528740     |      -       |       -       |    13.19  \n",
      "   3    |    440      |    0.531299     |      -       |       -       |    13.15  \n",
      "   3    |    480      |    0.527844     |      -       |       -       |    13.18  \n",
      "   3    |    503      |    0.504272     |      -       |       -       |    7.50   \n",
      "--------------------------------------------------------------------------------\n",
      "[[1056  669]\n",
      " [ 508 1797]]\n",
      "   3    |     -      |    0.513301    |   0.566140   |    70.80     |    69.77     |  177.38  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(bert_model, train_iter,\n",
    "      test_iter, optimizer, scheduler, epochs=3, evaluation=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
