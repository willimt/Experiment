{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.command.config import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging\n",
    "import processing\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from MyKu import processing\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = processing.get_HateXplain_train_data()\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "\n",
    "\n",
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))\n",
    "\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for sent, label in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        labels.append(label)\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "    attention_masks = torch.Tensor(attention_masks)\n",
    "    # print(attention_masks)\n",
    "    labels = torch.Tensor(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "\n",
    "train_inputs, train_masks, train_labels = preprocessing_for_bert(train_data)\n",
    "test_inputs, test_masks, test_labels = preprocessing_for_bert(test_data)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "test_iter = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyBertModel(nn.Module):\n",
    "\n",
    "    def __init__(self, class_size, num_layers, dropout, pretrained_name='bert-base-uncased'):\n",
    "\n",
    "        # 类继承的初始化，固定写法\n",
    "        super(MyBertModel, self).__init__()\n",
    "        # 加载HuggingFace的BertModel\n",
    "        # BertModel的最终输出维度默认为768\n",
    "        # return_dict=True 可以使BertModel的输出具有dict属性，即以 bert_output['last_hidden_state'] 方式调用\n",
    "\n",
    "        config = BertConfig.from_pretrained(pretrained_name)\n",
    "        config.output_attentions = True\n",
    "        config.return_dict = True\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name, config=config)\n",
    "        # 通过一个线性层将[CLS]标签对应的维度：768->class_size\n",
    "        # class_size 在SST-2情感分类任务中设置为：2\n",
    "        self.lstm = nn.LSTM(768, 100, num_layers=num_layers,\n",
    "                            bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(768, 768))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(768, 768))\n",
    "        # self.U = nn.Parameter(torch.Tensor(MAX_LEN, class_size))\n",
    "        # self.V = nn.Parameter(torch.Tensor(MAX_LEN, class_size))\n",
    "        # self.g = nn.Parameter(torch.Tensor(class_size))\n",
    "        # self.W_f = nn.Parameter(torch.Tensor(2 * MAX_LEN, class_size))\n",
    "        # self.bias = nn.Parameter(torch.Tensor(class_size))\n",
    "        self.decoder1 = nn.Linear(100 * 4, MAX_LEN)\n",
    "        self.decoder2 = nn.Linear(2 * MAX_LEN, class_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "        # nn.init.uniform_(self.U, -0.1, 0.1)\n",
    "        # nn.init.uniform_(self.V, -0.1, 0.1)\n",
    "        # nn.init.uniform_(self.g, -0.1, 0.1)\n",
    "        # nn.init.uniform_(self.W_f, -0.1, 0.1)\n",
    "        # nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(768, 100),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(100, class_size)\n",
    "        # )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取DataLoader中已经处理好的输入数据：\n",
    "        # input_ids :tensor类型，shape=batch_size*max_len   max_len为当前batch中的最大句长\n",
    "        # input_tyi :tensor类型，\n",
    "        # input_attn_mask :tensor类型，因为input_ids中存在大量[Pad]填充，attention mask将pad部分值置为0，让模型只关注非pad部分\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0] # last_hidden_state_cls (32, 60, 768)\n",
    "        bert_output, idxs = torch.max(last_hidden_state_cls, dim=2) # bert_output (32, 60)\n",
    "        # print(last_hidden_state_cls.shape)\n",
    "        w = torch.tanh(torch.matmul(last_hidden_state_cls, self.weight_W))  # w torch.Size([32, 60, 768])\n",
    "        self_matching = torch.matmul(w, self.weight_proj)   # w torch.Size([32, 60, 60])\n",
    "        att_score, idxs = torch.max(self_matching, dim=2)   # att_score torch.Size([32, 60])\n",
    "        self.lstm.flatten_parameters()\n",
    "        output_hidden, _ = self.lstm(last_hidden_state_cls) #output_hidden (32, 60, 200)\n",
    "        output = torch.cat((output_hidden[:,0,:], output_hidden[:,-1,:]), dim=1)    # output torch.Size([64, 400])\n",
    "        output = self.decoder1(output)      # output torch.Size([32, 60])\n",
    "        # self_matching_out torch.Size([32, 60])\n",
    "        self_matching_out = att_score.mul(output)\n",
    "        # outs = self.decoder2(self_matching_out)\n",
    "        f = torch.cat((bert_output, self_matching_out), dim=-1)\n",
    "        # outs = torch.softmax(torch.matmul(f, self.W_f) + self.bias, dim=1)\n",
    "        # f = self.relu(f)\n",
    "        outs = self.decoder2(f)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epoch=10):\n",
    "    bert_model = MyBertModel(2, 1, 0.5)\n",
    "    bert_model.to(DEVICE)\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = AdamW(\n",
    "        bert_model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        eps = 1e-8\n",
    "    )\n",
    "    total_steps = len(train_iter) * epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    return bert_model, optimizer, scheduler\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, test_iter, optimizer, scheduler, epochs=10, evaluation=None):\n",
    "    for num_epoch in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'每40个Batch':^9} | {'训练集 Loss':^12} | {'测试集 Loss':^10} | {'测试集准确率':^9} | {'时间':^9}\")\n",
    "        print(\"-\" * 80)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_att_masks, b_labels = tuple(t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "            model.zero_grad()\n",
    "            # b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            output = model(b_input_ids, b_att_masks)\n",
    "            # print(output.dtype)\n",
    "            b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            loss = loss_fn(output, b_labels.long())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if(step % 40 == 0 and step != 0) or (step == len(train_iter) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(f\"{num_epoch + 1:^7} | {step:^10}  | {batch_loss / batch_counts:^14.6f}  | {'-':^12} | {'-':^13} |  {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_iter)\n",
    "        print('-' * 80)\n",
    "\n",
    "        if evaluation:\n",
    "            test_loss, test_accuracy, f1_score = evaluate(model, test_iter)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{num_epoch + 1:^7} | {'-':^10} | {avg_train_loss:^14.6f} | {test_loss:^12.6f} | {test_accuracy:^12.2f} | {f1_score:^12.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    在每个epoch后验证集上评估model性能\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 准确率和误差\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    pred_y, true_y = [], []\n",
    "    # 验证集上的每个batch\n",
    "    for batch in test_dataloader:\n",
    "        # 放到GPU上\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(\n",
    "            t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "        # 计算结果，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            # 放到model里面去跑，返回验证集的ouput就是一行三列的\n",
    "            output = model(b_input_ids, b_attn_mask)\n",
    "        # 计算误差\n",
    "        loss = loss_fn(output, b_labels.long())\n",
    "        test_loss.append(loss.item())\n",
    "        pred = torch.argmax(output, dim=1).flatten()\n",
    "        # get预测结果，这里就是求每行最大的索引咯，然后用flatten打平成一维\n",
    "        # 计算准确率，这个就是俩比较，返回相同的个数, .cpu().numpy()就是把tensor从显卡上取出来然后转化为numpy类型的举证好用方法\n",
    "        # 最后mean因为直接bool形了，也就是如果预测和label一样那就返回1，正好是正确的个数，求平均就是准确率了\n",
    "        accuracy = (pred == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "        pred_y.extend(pred.tolist())\n",
    "        true_y.extend(b_labels.tolist())\n",
    "    # print(len(true_y))\n",
    "    # 计算整体的平均正确率和loss\n",
    "    print(metrics.confusion_matrix(true_y, pred_y))\n",
    "    val_loss = np.mean(test_loss)\n",
    "    val_accuracy = np.mean(test_accuracy)\n",
    "    f1_socre = metrics.f1_score(true_y, pred_y, average=\"macro\") * 100\n",
    "\n",
    "    return val_loss, val_accuracy, f1_socre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model, optimizer, scheduler = initialize_model(2)\n",
    "print(\"Start training and testing:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     40      |    0.664687     |      -       |       -       |    14.17  \n",
      "   1    |     80      |    0.647067     |      -       |       -       |    13.68  \n",
      "   1    |    120      |    0.632654     |      -       |       -       |    13.76  \n",
      "   1    |    160      |    0.598773     |      -       |       -       |    13.81  \n",
      "   1    |    200      |    0.616157     |      -       |       -       |    13.79  \n",
      "   1    |    240      |    0.610029     |      -       |       -       |    13.82  \n",
      "   1    |    280      |    0.590156     |      -       |       -       |    13.82  \n",
      "   1    |    320      |    0.583248     |      -       |       -       |    13.81  \n",
      "   1    |    360      |    0.578852     |      -       |       -       |    13.79  \n",
      "   1    |    400      |    0.555521     |      -       |       -       |    13.81  \n",
      "   1    |    440      |    0.560559     |      -       |       -       |    13.83  \n",
      "   1    |    480      |    0.583816     |      -       |       -       |    13.82  \n",
      "   1    |    503      |    0.591775     |      -       |       -       |    7.87   \n",
      "--------------------------------------------------------------------------------\n",
      "[[1223  547]\n",
      " [ 645 1615]]\n",
      "   1    |     -      |    0.601461    |   0.553877   |    70.42     |    70.14     |  185.05  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Experiment\\Ag\\HateXplain\\based_BERT-LSTM.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(bert_model, train_iter,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m       test_iter, optimizer, scheduler, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, evaluation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32md:\\Experiment\\Ag\\HateXplain\\based_BERT-LSTM.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, test_iter, optimizer, scheduler, epochs, evaluation)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/HateXplain/based_BERT-LSTM.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m(step \u001b[39m%\u001b[39m \u001b[39m40\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m step \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (step \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(train_iter) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    159\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 161\u001b[0m     adamw(params_with_grad,\n\u001b[0;32m    162\u001b[0m           grads,\n\u001b[0;32m    163\u001b[0m           exp_avgs,\n\u001b[0;32m    164\u001b[0m           exp_avg_sqs,\n\u001b[0;32m    165\u001b[0m           max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m           state_steps,\n\u001b[0;32m    167\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    168\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    169\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    170\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    172\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    173\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 218\u001b[0m func(params,\n\u001b[0;32m    219\u001b[0m      grads,\n\u001b[0;32m    220\u001b[0m      exp_avgs,\n\u001b[0;32m    221\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    222\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    223\u001b[0m      state_steps,\n\u001b[0;32m    224\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    225\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    226\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    228\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    229\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    230\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    231\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\adamw.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    260\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    262\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m param\u001b[39m.\u001b[39;49mmul_(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m lr \u001b[39m*\u001b[39;49m weight_decay)\n\u001b[0;32m    265\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    266\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train(bert_model, train_iter,\n",
    "      test_iter, optimizer, scheduler, epochs=3, evaluation=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
