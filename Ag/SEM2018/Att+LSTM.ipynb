{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from MyKu import processing\n",
    "from MyKu import training\n",
    "from MyKu import MHeadAttention\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.legacy import data\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from d2l import torch as d2l\n",
    "from torch.autograd import Variable\n",
    "from spacy.lang.en import English\n",
    "from sklearn import metrics\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.create_Sem2018()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):  # create a tokenizer function\n",
    "    \"\"\"\n",
    "    定义分词操作\n",
    "    \"\"\"\n",
    "    return processing.Pre_processing_tweets().tokenize_process(text)\n",
    "\n",
    "\n",
    "def DataLoader():\n",
    "\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer,\n",
    "                      lower=True, include_lengths=True, fix_length=20)\n",
    "    LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "    # 假如train.csv文件并不是只有两列，比如1、3列是review和polarity，2列是我们不需要的数据，\n",
    "    # 那么就要添加一个全是None的元组， fields列表存储的Field的顺序必须和csv文件中每一列的顺序对应，\n",
    "\n",
    "    train_fields = [(None, None), ('label', LABEL),  ('tweet', TEXT)]\n",
    "    # train_fields = [(None, None), (None, None), (None, None), (None, None), ('text', TEXT), ('task1', LABEL)]\n",
    "    train_data = data.TabularDataset(\n",
    "        path='D:/Experiment/datasets/SEM2018/train.tsv',\n",
    "        # path='D:/Experiment/datasets/EXIST2021/train.tsv',\n",
    "        format='tsv',\n",
    "        fields=train_fields,\n",
    "        skip_header=True  # 是否跳过文件的第一行\n",
    "    )\n",
    "    test_fields = [(None, None), ('label', LABEL),  ('tweet', TEXT)]\n",
    "    # test_fields = [(None, None), (None, None), (None, None), (None, None), ('text', TEXT), ('task1', LABEL)]\n",
    "    test_data = data.TabularDataset(\n",
    "        path='D:/Experiment/datasets/SEM2018/test.tsv',\n",
    "        # path='D:/Experiment/datasets/EXIST2021/test.tsv',\n",
    "        format='tsv',\n",
    "        fields=test_fields,\n",
    "        skip_header=True  # 是否跳过文件的第一行\n",
    "    )\n",
    "    return train_data, test_data, TEXT, LABEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, TEXT, LABEL = DataLoader()\n",
    "\n",
    "vectors = Vectors(name='glove.6B.300d.txt', cache=processing.EMBEDDING_PATH)\n",
    "\n",
    "TEXT.build_vocab(train_data,  # 建词表是用训练集建，不要用验证集和测试集\n",
    "                  max_size=400000, # 单词表容量\n",
    "                  vectors=vectors, # 还有'glove.840B.300d'已经很多可以选\n",
    "                  unk_init=torch.Tensor.normal_ # 初始化train_data中不存在预训练词向量词表中的单词\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，\n",
    "    # num_hiddens/num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "#@save\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, query_size, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # queries，keys，values的形状:\n",
    "        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # valid_lens　的形状:\n",
    "        # (batch_size，)或(batch_size，查询的个数)\n",
    "        # 经过变换后，输出的queries，keys，values　的形状:\n",
    "        # (batch_size*num_heads，查询或者“键－值”对的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "        # output的形状:(batch_size*num_heads，查询的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        output = self.W_o(output_concat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MModel(nn.Module):\n",
    "    def __init__(self,vocab_size, embed_size, num_hiddens, output_dim, max_length, num_layers, dropout, **kwargs):\n",
    "        super(MModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.LSTM = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.n_class = output_dim\n",
    "        self.max_length = max_length\n",
    "        self.attention = MultiHeadAttention(max_length, max_length, max_length, max_length, 5, 0.5)\n",
    "        self.decoder1 = nn.Linear(num_hiddens * 4, max_length)\n",
    "        self.decoder2 = nn.Linear(num_hiddens, self.n_class)\n",
    "        self.dense = nn.Linear(embed_size, max_length)\n",
    "        self.U = nn.Parameter(torch.Tensor(max_length, output_dim))\n",
    "        self.V = nn.Parameter(torch.Tensor(num_hiddens, output_dim))\n",
    "        self.g = nn.Parameter(torch.Tensor(max_length))\n",
    "        self.W_f = nn.Parameter(torch.Tensor(output_dim, output_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        nn.init.uniform_(self.U, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.V, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.g, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.W_f, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "    \n",
    "    def forward(self, inputs, text_lens): #inputs torch.Size([64, 40])\n",
    "        input = self.embedding(inputs.permute(1, 0)) #input torch.Size([64, 40, 300])\n",
    "        valid_lens = text_lens\n",
    "        att_input = self.dense(input)\n",
    "        self_attention = self.attention(att_input, att_input, att_input, valid_lens)  # self_attention torch.size([64, 40, 40])\n",
    "        # print(self_attention.shape)\n",
    "        att_score, idxs = torch.max(self_attention, dim=1)   # att_score torch.Size([64, 40])\n",
    "        self.LSTM.flatten_parameters()\n",
    "        outputs, _ = self.LSTM(input)    #outputs torch.Size([64, 40, 200])\n",
    "        output = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)    # output torch.Size([64, 400])\n",
    "        output = self.decoder1(output)      # output torch.Size([64, 100])\n",
    "        # outs = self.decoder2(output)\n",
    "        f_a = torch.matmul(att_score, self.U)   # output torch.Size([64, 2])\n",
    "        f_b = torch.matmul(output, self.V)  # output torch.Size([64, 2])\n",
    "        f = f_a.mul(f_b) + self.g   # output torch.Size([64, 2])\n",
    "        outs = torch.softmax(torch.matmul(f, self.W_f) + self.bias, dim=1)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_iter, optimizer, loss, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_sample = 0\n",
    "    correct = 0\n",
    "    for batch in tqdm(train_iter, desc=f\"Training Epoch {epoch}\", colour='red'):\n",
    "        optimizer.zero_grad()\n",
    "        text, text_len = batch.tweet\n",
    "        label = batch.label\n",
    "        # text, text_len = batch.text\n",
    "        # label = batch.task1\n",
    "        output = model(text, text_len)\n",
    "        pred_y = torch.argmax(output, dim=1)\n",
    "        correct += torch.sum(pred_y == label)\n",
    "        l = loss(output, label)\n",
    "        l.backward()\n",
    "        epoch_loss += l.item()\n",
    "        num_sample += len(batch)\n",
    "        optimizer.step()\n",
    "    print(\n",
    "        f'\\tTrain Loss: {epoch_loss / num_sample:.3f} | Train Acc: {correct.float() / num_sample* 100:.2f}%')\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    true_y, pred_y = [], []\n",
    "    for batch in tqdm(test_iter, desc=f\"Testing\", colour='green'):\n",
    "        text, text_len = batch.tweet\n",
    "        label = batch.label\n",
    "        # text, text_len = batch.text\n",
    "        # label = batch.task1\n",
    "        with torch.no_grad():\n",
    "            output = model(text, text_len)\n",
    "            pred_y.extend(output.argmax(dim=1).tolist())\n",
    "            true_y.extend(label.tolist())\n",
    "    print(metrics.confusion_matrix(true_y, pred_y))\n",
    "    print(metrics.classification_report(true_y, pred_y))\n",
    "    print(f'Acc : {metrics.accuracy_score(true_y, pred_y)}\\t F1: {metrics.f1_score(true_y, pred_y, average=\"macro\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, output_dim, max_length, num_layers, dropout = 100, 2, 20, 2, 0.7\n",
    "model = MModel(len(TEXT.vocab), 300, num_hiddens, output_dim, max_length, num_layers, dropout)\n",
    "model.to(DEVICE)\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "    # print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(300)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(300)\n",
    "\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits((train_data, test_data), batch_size=64, sort_within_batch=True, sort_key=lambda x: len(x.tweet), device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 62.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 90.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 249.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[319 154]\n",
      " [140 171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       473\n",
      "           1       0.53      0.55      0.54       311\n",
      "\n",
      "    accuracy                           0.62       784\n",
      "   macro avg       0.61      0.61      0.61       784\n",
      "weighted avg       0.63      0.62      0.63       784\n",
      "\n",
      "Acc : 0.625\t F1: 0.6111426026398898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 73.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 90.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 231.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[309 164]\n",
      " [134 177]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.67       473\n",
      "           1       0.52      0.57      0.54       311\n",
      "\n",
      "    accuracy                           0.62       784\n",
      "   macro avg       0.61      0.61      0.61       784\n",
      "weighted avg       0.63      0.62      0.62       784\n",
      "\n",
      "Acc : 0.6198979591836735\t F1: 0.6088086371795216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 69.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 207.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279 194]\n",
      " [111 200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       473\n",
      "           1       0.51      0.64      0.57       311\n",
      "\n",
      "    accuracy                           0.61       784\n",
      "   macro avg       0.61      0.62      0.61       784\n",
      "weighted avg       0.63      0.61      0.62       784\n",
      "\n",
      "Acc : 0.610969387755102\t F1: 0.6069787891488541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 73.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 196.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[304 169]\n",
      " [132 179]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67       473\n",
      "           1       0.51      0.58      0.54       311\n",
      "\n",
      "    accuracy                           0.62       784\n",
      "   macro avg       0.61      0.61      0.61       784\n",
      "weighted avg       0.62      0.62      0.62       784\n",
      "\n",
      "Acc : 0.6160714285714286\t F1: 0.606057115574987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 74.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 202.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[221 252]\n",
      " [ 77 234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.47      0.57       473\n",
      "           1       0.48      0.75      0.59       311\n",
      "\n",
      "    accuracy                           0.58       784\n",
      "   macro avg       0.61      0.61      0.58       784\n",
      "weighted avg       0.64      0.58      0.58       784\n",
      "\n",
      "Acc : 0.5803571428571429\t F1: 0.5802417300935576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 76.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 90.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 231.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[285 188]\n",
      " [118 193]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.60      0.65       473\n",
      "           1       0.51      0.62      0.56       311\n",
      "\n",
      "    accuracy                           0.61       784\n",
      "   macro avg       0.61      0.61      0.60       784\n",
      "weighted avg       0.63      0.61      0.61       784\n",
      "\n",
      "Acc : 0.6096938775510204\t F1: 0.6042441998574708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 75.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 90.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 236.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[303 170]\n",
      " [129 182]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67       473\n",
      "           1       0.52      0.59      0.55       311\n",
      "\n",
      "    accuracy                           0.62       784\n",
      "   macro avg       0.61      0.61      0.61       784\n",
      "weighted avg       0.63      0.62      0.62       784\n",
      "\n",
      "Acc : 0.6186224489795918\t F1: 0.6093164337558229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 75.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 236.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[269 204]\n",
      " [109 202]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.57      0.63       473\n",
      "           1       0.50      0.65      0.56       311\n",
      "\n",
      "    accuracy                           0.60       784\n",
      "   macro avg       0.60      0.61      0.60       784\n",
      "weighted avg       0.63      0.60      0.60       784\n",
      "\n",
      "Acc : 0.6007653061224489\t F1: 0.597828135575998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 75.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 220.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[336 137]\n",
      " [148 163]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       473\n",
      "           1       0.54      0.52      0.53       311\n",
      "\n",
      "    accuracy                           0.64       784\n",
      "   macro avg       0.62      0.62      0.62       784\n",
      "weighted avg       0.63      0.64      0.64       784\n",
      "\n",
      "Acc : 0.6364795918367347\t F1: 0.617872956097461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 75.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 240.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[321 152]\n",
      " [143 168]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.69       473\n",
      "           1       0.53      0.54      0.53       311\n",
      "\n",
      "    accuracy                           0.62       784\n",
      "   macro avg       0.61      0.61      0.61       784\n",
      "weighted avg       0.63      0.62      0.62       784\n",
      "\n",
      "Acc : 0.6237244897959183\t F1: 0.6088267678313801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 75.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 220.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[299 174]\n",
      " [125 186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.63      0.67       473\n",
      "           1       0.52      0.60      0.55       311\n",
      "\n",
      "    accuracy                           0.62       784\n",
      "   macro avg       0.61      0.62      0.61       784\n",
      "weighted avg       0.63      0.62      0.62       784\n",
      "\n",
      "Acc : 0.6186224489795918\t F1: 0.6105315449577745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 68.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 209.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280 193]\n",
      " [112 199]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.59      0.65       473\n",
      "           1       0.51      0.64      0.57       311\n",
      "\n",
      "    accuracy                           0.61       784\n",
      "   macro avg       0.61      0.62      0.61       784\n",
      "weighted avg       0.63      0.61      0.62       784\n",
      "\n",
      "Acc : 0.610969387755102\t F1: 0.6067719681957588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 63.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 202.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[228 245]\n",
      " [ 77 234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.48      0.59       473\n",
      "           1       0.49      0.75      0.59       311\n",
      "\n",
      "    accuracy                           0.59       784\n",
      "   macro avg       0.62      0.62      0.59       784\n",
      "weighted avg       0.64      0.59      0.59       784\n",
      "\n",
      "Acc : 0.5892857142857143\t F1: 0.5892616576095798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 63.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.007 | Train Acc: 89.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 216.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226 247]\n",
      " [ 78 233]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.48      0.58       473\n",
      "           1       0.49      0.75      0.59       311\n",
      "\n",
      "    accuracy                           0.59       784\n",
      "   macro avg       0.61      0.61      0.59       784\n",
      "weighted avg       0.64      0.59      0.58       784\n",
      "\n",
      "Acc : 0.5854591836734694\t F1: 0.5854261340987004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 64.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.007 | Train Acc: 89.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 232.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[312 161]\n",
      " [128 183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68       473\n",
      "           1       0.53      0.59      0.56       311\n",
      "\n",
      "    accuracy                           0.63       784\n",
      "   macro avg       0.62      0.62      0.62       784\n",
      "weighted avg       0.64      0.63      0.63       784\n",
      "\n",
      "Acc : 0.6313775510204082\t F1: 0.6211198715751276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 64.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 193.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[285 188]\n",
      " [114 197]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.60      0.65       473\n",
      "           1       0.51      0.63      0.57       311\n",
      "\n",
      "    accuracy                           0.61       784\n",
      "   macro avg       0.61      0.62      0.61       784\n",
      "weighted avg       0.63      0.61      0.62       784\n",
      "\n",
      "Acc : 0.6147959183673469\t F1: 0.6098808393968154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 66.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 202.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334 139]\n",
      " [145 166]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70       473\n",
      "           1       0.54      0.53      0.54       311\n",
      "\n",
      "    accuracy                           0.64       784\n",
      "   macro avg       0.62      0.62      0.62       784\n",
      "weighted avg       0.64      0.64      0.64       784\n",
      "\n",
      "Acc : 0.6377551020408163\t F1: 0.6203208556149733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 64.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 223.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[281 192]\n",
      " [110 201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       473\n",
      "           1       0.51      0.65      0.57       311\n",
      "\n",
      "    accuracy                           0.61       784\n",
      "   macro avg       0.62      0.62      0.61       784\n",
      "weighted avg       0.64      0.61      0.62       784\n",
      "\n",
      "Acc : 0.6147959183673469\t F1: 0.610742845117845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 209.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[314 159]\n",
      " [132 179]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68       473\n",
      "           1       0.53      0.58      0.55       311\n",
      "\n",
      "    accuracy                           0.63       784\n",
      "   macro avg       0.62      0.62      0.62       784\n",
      "weighted avg       0.63      0.63      0.63       784\n",
      "\n",
      "Acc : 0.6288265306122449\t F1: 0.6174846713199011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|\u001b[31m██████████\u001b[0m| 60/60 [00:00<00:00, 71.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 91.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 13/13 [00:00<00:00, 245.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279 194]\n",
      " [111 200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       473\n",
      "           1       0.51      0.64      0.57       311\n",
      "\n",
      "    accuracy                           0.61       784\n",
      "   macro avg       0.61      0.62      0.61       784\n",
      "weighted avg       0.63      0.61      0.62       784\n",
      "\n",
      "Acc : 0.610969387755102\t F1: 0.6069787891488541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.0001, 20\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(model, train_iter, optimizer, loss, epoch)\n",
    "    test(model, test_iter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
