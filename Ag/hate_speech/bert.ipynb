{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.command.config import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging\n",
    "import processing\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from MyKu import processing\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19826"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = processing.get_hate_train_data()\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "\n",
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for sent, label in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        labels.append(label)\n",
    "    input_ids = torch.Tensor(input_ids)\n",
    "    attention_masks = torch.Tensor(attention_masks)\n",
    "    # print(attention_masks)\n",
    "    labels = torch.Tensor(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_inputs, train_masks, train_labels = preprocessing_for_bert(train_data)\n",
    "test_inputs, test_masks, test_labels = preprocessing_for_bert(test_data)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "test_iter = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertModel(nn.Module):\n",
    "\n",
    "    def __init__(self, class_size, pretrained_name='bert-base-uncased'):\n",
    "\n",
    "        # 类继承的初始化，固定写法\n",
    "        super(MyBertModel, self).__init__()\n",
    "        # 加载HuggingFace的BertModel\n",
    "        # BertModel的最终输出维度默认为768\n",
    "        # return_dict=True 可以使BertModel的输出具有dict属性，即以 bert_output['last_hidden_state'] 方式调用\n",
    "\n",
    "        config = BertConfig.from_pretrained(pretrained_name)\n",
    "        config.output_attentions = True\n",
    "        config.return_dict = True\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name, config=config)\n",
    "        # 通过一个线性层将[CLS]标签对应的维度：768->class_size\n",
    "        # class_size 在SST-2情感分类任务中设置为：2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, class_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取DataLoader中已经处理好的输入数据：\n",
    "        # input_ids :tensor类型，shape=batch_size*max_len   max_len为当前batch中的最大句长\n",
    "        # input_tyi :tensor类型，\n",
    "        # input_attn_mask :tensor类型，因为input_ids中存在大量[Pad]填充，attention mask将pad部分值置为0，让模型只关注非pad部分\n",
    "\n",
    "        # 将三者输入进模型，如果想知道模型内部如何运作，前面的蛆以后再来探索吧~\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # bert_output 分为两个部分：\n",
    "        #   last_hidden_state:最后一个隐层的值\n",
    "        #   pooler output:对应的是[CLS]的输出,用于分类任务\n",
    "        # 通过线性层将维度：768->2\n",
    "        # categories_numberic：tensor类型，shape=batch_size*class_size，用于后续的CrossEntropy计算\n",
    "        categories_numberic = self.classifier(last_hidden_state_cls)\n",
    "        return categories_numberic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epoch=3):\n",
    "    bert_model = MyBertModel(2)\n",
    "    bert_model.to(DEVICE)\n",
    "    learning_rate = 1e-5\n",
    "    optimizer = AdamW(\n",
    "        bert_model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    total_steps = len(train_iter) * epoch\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    return bert_model, optimizer, scheduler\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, test_iter, optimizer, scheduler, epochs=10, evaluation=None):\n",
    "    for num_epoch in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'每40个Batch':^9} | {'训练集 Loss':^12} | {'测试集 Loss':^10} | {'测试集准确率':^9} | {'时间':^9}\")\n",
    "        print(\"-\" * 80)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_att_masks, b_labels = tuple(\n",
    "                t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "            model.zero_grad()\n",
    "            # b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            output = model(b_input_ids, b_att_masks)\n",
    "            # print(output.dtype)\n",
    "            b_input_ids = b_input_ids.to(dtype=torch.int32)\n",
    "            loss = loss_fn(output, b_labels.long())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if(step % 40 == 0 and step != 0) or (step == len(train_iter) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                print(\n",
    "                    f\"{num_epoch + 1:^7} | {step:^10}  | {batch_loss / batch_counts:^14.6f}  | {'-':^12} | {'-':^13} |  {time_elapsed:^9.2f}\")\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_iter)\n",
    "        print('-' * 80)\n",
    "\n",
    "        if evaluation:\n",
    "            test_loss, test_accuracy, f1_score = evaluate(model, test_iter)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{num_epoch + 1:^7} | {'-':^10} | {avg_train_loss:^14.6f} | {test_loss:^12.6f} | {test_accuracy:^12.2f} | {f1_score:^12.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    在每个epoch后验证集上评估model性能\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # 准确率和误差\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    pred_y, true_y = [], []\n",
    "    # 验证集上的每个batch\n",
    "    for batch in test_dataloader:\n",
    "        # 放到GPU上\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(\n",
    "            t.to(DEVICE, dtype=torch.int32) for t in batch)\n",
    "        # 计算结果，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            # 放到model里面去跑，返回验证集的ouput就是一行三列的\n",
    "            output = model(b_input_ids, b_attn_mask)\n",
    "        # 计算误差\n",
    "        loss = loss_fn(output, b_labels.long())\n",
    "        test_loss.append(loss.item())\n",
    "        pred = torch.argmax(output, dim=1).flatten()\n",
    "        # get预测结果，这里就是求每行最大的索引咯，然后用flatten打平成一维\n",
    "        # 计算准确率，这个就是俩比较，返回相同的个数, .cpu().numpy()就是把tensor从显卡上取出来然后转化为numpy类型的举证好用方法\n",
    "        # 最后mean因为直接bool形了，也就是如果预测和label一样那就返回1，正好是正确的个数，求平均就是准确率了\n",
    "        accuracy = (pred == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "        pred_y.extend(pred.tolist())\n",
    "        true_y.extend(b_labels.tolist())\n",
    "    # print(len(true_y))\n",
    "    # 计算整体的平均正确率和loss\n",
    "    print(metrics.confusion_matrix(true_y, pred_y))\n",
    "    val_loss = np.mean(test_loss)\n",
    "    val_accuracy = np.mean(test_accuracy)\n",
    "    f1_socre = metrics.f1_score(true_y, pred_y, average=\"macro\") * 100\n",
    "\n",
    "    return val_loss, val_accuracy, f1_socre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model, optimizer, scheduler = initialize_model(2)\n",
    "print(\"Start training and testing:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     40      |    0.089279     |      -       |       -       |    14.19  \n",
      "   1    |     80      |    0.065796     |      -       |       -       |    13.52  \n",
      "   1    |    120      |    0.069781     |      -       |       -       |    13.66  \n",
      "   1    |    160      |    0.067174     |      -       |       -       |    13.50  \n",
      "   1    |    200      |    0.080881     |      -       |       -       |    13.66  \n",
      "   1    |    240      |    0.070580     |      -       |       -       |    13.75  \n",
      "   1    |    280      |    0.087914     |      -       |       -       |    13.60  \n",
      "   1    |    320      |    0.089607     |      -       |       -       |    13.49  \n",
      "   1    |    360      |    0.070921     |      -       |       -       |    13.57  \n",
      "   1    |    400      |    0.092676     |      -       |       -       |    13.49  \n",
      "   1    |    440      |    0.072508     |      -       |       -       |    13.62  \n",
      "   1    |    480      |    0.060396     |      -       |       -       |    13.72  \n",
      "   1    |    520      |    0.073351     |      -       |       -       |    13.50  \n",
      "   1    |    560      |    0.070045     |      -       |       -       |    13.69  \n",
      "   1    |    600      |    0.089816     |      -       |       -       |    13.49  \n",
      "   1    |    619      |    0.111867     |      -       |       -       |    6.40   \n",
      "--------------------------------------------------------------------------------\n",
      "[[ 693  106]\n",
      " [  85 4073]]\n",
      "   1    |     -      |    0.077812    |   0.112842   |    96.15     |    92.80     |  224.51  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |     40      |    0.085814     |      -       |       -       |    13.85  \n",
      "   2    |     80      |    0.056304     |      -       |       -       |    13.49  \n",
      "   2    |    120      |    0.093695     |      -       |       -       |    13.54  \n",
      "   2    |    160      |    0.085921     |      -       |       -       |    13.72  \n",
      "   2    |    200      |    0.073533     |      -       |       -       |    13.46  \n",
      "   2    |    240      |    0.065572     |      -       |       -       |    13.66  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Experiment\\Ag\\hate_speech\\bert.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(bert_model, train_iter,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m       test_iter, optimizer, scheduler, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, evaluation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32md:\\Experiment\\Ag\\hate_speech\\bert.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, test_iter, optimizer, scheduler, epochs, evaluation)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m b_input_ids \u001b[39m=\u001b[39m b_input_ids\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, b_labels\u001b[39m.\u001b[39mlong())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m batch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiment/Ag/hate_speech/bert.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train(bert_model, train_iter,\n",
    "      test_iter, optimizer, scheduler, epochs=3, evaluation=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
