{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.legacy import data\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from d2l import torch as d2l\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "sys.path.append(\"D:/Experiment\")\n",
    "from tqdm import tqdm\n",
    "from MyKu import training\n",
    "from MyKu import processing\n",
    "from torchtext.vocab import Vectors\n",
    "from spacy.lang.en import English\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.create_OLID()\n",
    "# processing.set_olid_train_data(processing.ORIGIN_DATASET_PATH + '/OLID')\n",
    "# processing.set_olid_testA_data(processing.ORIGIN_DATASET_PATH + '/OLID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = processing.get_OLID_train_data(processing.OLID_DATASET + '/train.tsv')\n",
    "# test_data = processing.get_OLID_testA_data(\n",
    "#     processing.OLID_DATASET + '/testA.tsv', processing.OLID_DATASET + '/labels-levela.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = English()\n",
    "\n",
    "def tokenizer(text):  # create a tokenizer function\n",
    "    \"\"\"\n",
    "    定义分词操作\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def DataLoader():\n",
    "    def tokenize(x): return x.split()\n",
    "\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True)\n",
    "    LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "    # 假如train.csv文件并不是只有两列，比如1、3列是review和polarity，2列是我们不需要的数据，\n",
    "    # 那么就要添加一个全是None的元组， fields列表存储的Field的顺序必须和csv文件中每一列的顺序对应，\n",
    "\n",
    "    train_fields = [(None, None), ('tweet', TEXT), ('subtask_a', LABEL)]\n",
    "    # train_fields = [(None, None), (None, None), (None, None), (None, None), ('text', TEXT), ('task1', LABEL)]\n",
    "    train_data= data.TabularDataset(\n",
    "    path='D:/Experiment/datasets/OLID/train.tsv',\n",
    "    # path='D:/Experiment/datasets/EXIST2021/train.tsv',\n",
    "    format='tsv',\n",
    "    fields=train_fields,\n",
    "    skip_header=True  # 是否跳过文件的第一行\n",
    "    )\n",
    "    test_fields = [(None, None), ('tweet', TEXT), ('label', LABEL)]\n",
    "    # test_fields = [(None, None), (None, None), (None, None), (None, None), ('text', TEXT), ('task1', LABEL)]\n",
    "    test_data= data.TabularDataset(\n",
    "    path='D:/Experiment/datasets/OLID/testA.tsv',\n",
    "    # path='D:/Experiment/datasets/EXIST2021/test.tsv',\n",
    "    format='tsv',\n",
    "    fields=test_fields,\n",
    "    skip_header=True  # 是否跳过文件的第一行\n",
    "    )\n",
    "    return train_data, test_data, TEXT, LABEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, TEXT, LABEL = DataLoader()\n",
    "# x1, x2 = 1, 1\n",
    "# for index in train_data:\n",
    "#     index.task1 = 1 if index.task1 == 'sexist' else 0\n",
    "\n",
    "# for index in test_data:\n",
    "#     index.task1 = 1 if index.task1 == 'sexist' else 0\n",
    "\n",
    "# temp_examples = []\n",
    "# for index in train_data.examples:\n",
    "#     if len(index.text) == 0:\n",
    "#         continue\n",
    "#     if x1 > 3437:\n",
    "#         break\n",
    "#     else:\n",
    "#         x1 += 1\n",
    "#     temp_examples.append(index)\n",
    "# train_data.examples = temp_examples\n",
    "# temp_examples = []\n",
    "# for index in test_data.examples:\n",
    "#     if len(index.text) == 0:\n",
    "#         continue\n",
    "#     if x2 > 1000:\n",
    "#         break\n",
    "#     else:\n",
    "#         x2 += 1\n",
    "#     temp_examples.append(index)\n",
    "# test_data.examples = temp_examples\n",
    "\n",
    "vectors = Vectors(name='glove.6B.300d.txt', cache=processing.EMBEDDING_PATH)\n",
    "\n",
    "TEXT.build_vocab(train_data,  # 建词表是用训练集建，不要用验证集和测试集\n",
    "                  max_size=400000, # 单词表容量\n",
    "                  vectors=vectors, # 还有'glove.840B.300d'已经很多可以选\n",
    "                  unk_init=torch.Tensor.normal_ # 初始化train_data中不存在预训练词向量词表中的单词\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@user',\n",
       " 'she',\n",
       " 'should',\n",
       " 'ask',\n",
       " 'a',\n",
       " 'few',\n",
       " 'native',\n",
       " 'americans',\n",
       " 'what',\n",
       " 'their',\n",
       " 'take',\n",
       " 'on',\n",
       " 'this',\n",
       " 'is',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectors.stoi['apples']\n",
    "# vectors.itos[253]\n",
    "train_data[0].tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiRNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "#         super(BiRNN, self).__init__(**kwargs)\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "#         self.encoder = nn.LSTM(\n",
    "#             embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, dropout=0.8)\n",
    "#         self.decoder = nn.Linear(num_hiddens * 4, 2)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         embeddings = self.embedding(inputs)\n",
    "#         # print(embeddings.shape)\n",
    "#         self.encoder.flatten_parameters()\n",
    "#         outputs, _ = self.encoder(embeddings)\n",
    "#         # print(outputs.shape)\n",
    "#         encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "#         outs = self.decoder(encoding)\n",
    "#         # print(outs.shape)\n",
    "#         return outs\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_hidden, output_dim, num_layers, dropout, pad_idx):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_size, num_hidden, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hiddens = num_hidden\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # self.fc = nn.Linear(num_hidden * 2, num_hidden)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(num_hidden * 2, output_dim)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(self.num_layers * 2, batch_size, self.num_hiddens)),\n",
    "            Variable(torch.zeros(self.num_layers * 2, batch_size, self.num_hiddens)))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "        batch_size = text.shape[1]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "        h_0, c_0 = h_0.to(DEVICE), c_0.to(DEVICE)\n",
    "        embedded = self.embedding(text.permute(1, 0))\n",
    "        # print('embedded size', embedded.shape)\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_len.cpu(), batch_first=True)\n",
    "        output_packed, (h_n, c_n) = self.lstm(packed_embedded, (h_0, c_0))\n",
    "        # print('output_packed size', output_packed.data.shape)\n",
    "        output_unpacked, output_len = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        # print('output_unpacked size', output_unpacked.data.shape)\n",
    "        output = output_unpacked[:, -1, :]\n",
    "        # rel = self.relu(output)\n",
    "        # dence = self.fc(rel)\n",
    "        # drop = self.dropout(dence)\n",
    "        preds = self.classifier(output)\n",
    "        # preds = F.softmax(preds, dim=1)\n",
    "        return preds\n",
    "\n",
    "# example shape\n",
    "# text size torch.Size([11, 32])\n",
    "# embedded size torch.Size([32, 11, 300])\n",
    "# batch size 32\n",
    "# output_packed size torch.Size([352, 200])\n",
    "# output_unpacked size torch.Size([32, 11, 200])\n",
    "# torch.Size([32, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(20335, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 200, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (classifier): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = torch.ones((4, 6), dtype=torch.long).to(device=DEVICE)\n",
    "model = BiLSTM(len(TEXT.vocab), 300, 200, 2, 2, 0.5, 1)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "# print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(300)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, test_data), batch_size=64, sort_within_batch=True, sort_key=lambda x : len(x.tweet), device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, optimizer, loss, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_sample = 0\n",
    "    correct = 0\n",
    "    for batch in tqdm(train_iter, desc=f\"Training Epoch {epoch}\", colour='red'):\n",
    "        optimizer.zero_grad()\n",
    "        text, text_len = batch.tweet\n",
    "        label = batch.subtask_a\n",
    "        # text, text_len = batch.text\n",
    "        # label = batch.task1\n",
    "        # print(text_len.shape)\n",
    "        output = model(text, text_len)\n",
    "        pred_y = torch.argmax(output, dim=1)\n",
    "        correct += torch.sum(pred_y == label)\n",
    "        l = loss(output, label)\n",
    "        l.backward()\n",
    "        epoch_loss += l.item()\n",
    "        num_sample += len(batch)\n",
    "        optimizer.step()\n",
    "    print(\n",
    "        f'\\tTrain Loss: {epoch_loss / num_sample:.3f} | Train Acc: {correct.float() / num_sample* 100:.2f}%')\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    true_y, pred_y = [], []\n",
    "    for batch in tqdm(test_iter, desc=f\"Testing\", colour='green'):\n",
    "        text, text_len = batch.tweet\n",
    "        label = batch.label\n",
    "        # text, text_len = batch.text\n",
    "        # label = batch.task1\n",
    "        with torch.no_grad():\n",
    "            output = model(text, text_len)\n",
    "            pred_y.extend(output.argmax(dim=1).tolist())\n",
    "            true_y.extend(label.tolist())\n",
    "    print(metrics.confusion_matrix(true_y, pred_y))\n",
    "    print(metrics.classification_report(true_y, pred_y))\n",
    "    print(f'Acc : {metrics.accuracy_score(true_y, pred_y)}\\t F1: {metrics.f1_score(true_y, pred_y, average=\"macro\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:04<00:00, 50.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.010 | Train Acc: 62.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 131.95it/s]\n",
      "d:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[620   0]\n",
      " [240   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84       620\n",
      "           1       0.00      0.00      0.00       240\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.36      0.50      0.42       860\n",
      "weighted avg       0.52      0.72      0.60       860\n",
      "\n",
      "Acc : 0.7209302325581395\t F1: 0.4189189189189189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 53.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.010 | Train Acc: 67.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 135.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[615   5]\n",
      " [238   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.99      0.84       620\n",
      "           1       0.29      0.01      0.02       240\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.50      0.50      0.43       860\n",
      "weighted avg       0.60      0.72      0.61       860\n",
      "\n",
      "Acc : 0.7174418604651163\t F1: 0.4256124409409864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 53.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.009 | Train Acc: 70.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 131.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[598  22]\n",
      " [232   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.82       620\n",
      "           1       0.27      0.03      0.06       240\n",
      "\n",
      "    accuracy                           0.70       860\n",
      "   macro avg       0.49      0.50      0.44       860\n",
      "weighted avg       0.59      0.70      0.61       860\n",
      "\n",
      "Acc : 0.7046511627906977\t F1: 0.44204342273307784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 54.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.009 | Train Acc: 72.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 141.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[595  25]\n",
      " [231   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.82       620\n",
      "           1       0.26      0.04      0.07       240\n",
      "\n",
      "    accuracy                           0.70       860\n",
      "   macro avg       0.49      0.50      0.44       860\n",
      "weighted avg       0.59      0.70      0.61       860\n",
      "\n",
      "Acc : 0.7023255813953488\t F1: 0.44432666000343257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 54.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.008 | Train Acc: 74.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 129.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[596  24]\n",
      " [231   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.82       620\n",
      "           1       0.27      0.04      0.07       240\n",
      "\n",
      "    accuracy                           0.70       860\n",
      "   macro avg       0.50      0.50      0.44       860\n",
      "weighted avg       0.60      0.70      0.61       860\n",
      "\n",
      "Acc : 0.7034883720930233\t F1: 0.44485369502646627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 53.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.008 | Train Acc: 74.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 138.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[611   9]\n",
      " [234   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.99      0.83       620\n",
      "           1       0.40      0.03      0.05       240\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.56      0.51      0.44       860\n",
      "weighted avg       0.63      0.72      0.61       860\n",
      "\n",
      "Acc : 0.7174418604651163\t F1: 0.44059425818108816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 53.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.007 | Train Acc: 77.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 141.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[609  11]\n",
      " [233   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.98      0.83       620\n",
      "           1       0.39      0.03      0.05       240\n",
      "\n",
      "    accuracy                           0.72       860\n",
      "   macro avg       0.56      0.51      0.44       860\n",
      "weighted avg       0.63      0.72      0.62       860\n",
      "\n",
      "Acc : 0.7162790697674418\t F1: 0.4436844505243958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 53.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.007 | Train Acc: 78.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 136.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[599  21]\n",
      " [230  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.97      0.83       620\n",
      "           1       0.32      0.04      0.07       240\n",
      "\n",
      "    accuracy                           0.71       860\n",
      "   macro avg       0.52      0.50      0.45       860\n",
      "weighted avg       0.61      0.71      0.62       860\n",
      "\n",
      "Acc : 0.708139534883721\t F1: 0.4502889128270165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 54.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.007 | Train Acc: 80.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 131.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[600  20]\n",
      " [228  12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.97      0.83       620\n",
      "           1       0.38      0.05      0.09       240\n",
      "\n",
      "    accuracy                           0.71       860\n",
      "   macro avg       0.55      0.51      0.46       860\n",
      "weighted avg       0.63      0.71      0.62       860\n",
      "\n",
      "Acc : 0.7116279069767442\t F1: 0.4584822879428014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|\u001b[31m██████████\u001b[0m| 207/207 [00:03<00:00, 53.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.006 | Train Acc: 81.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 14/14 [00:00<00:00, 124.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[604  16]\n",
      " [230  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.97      0.83       620\n",
      "           1       0.38      0.04      0.08       240\n",
      "\n",
      "    accuracy                           0.71       860\n",
      "   macro avg       0.55      0.51      0.45       860\n",
      "weighted avg       0.63      0.71      0.62       860\n",
      "\n",
      "Acc : 0.713953488372093\t F1: 0.4529997621288434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.0001, 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(model, train_iter, optimizer, loss, epoch)\n",
    "    test(model, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
