{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging\n",
    "import processing\n",
    "from sklearn import metrics\n",
    "import MyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练准备阶段，设置超参数和全局变量\n",
    "file_name = 'readme.md'\n",
    "batch_size = 16\n",
    "num_epoch = 10  # 训练轮次\n",
    "check_step = 1  # 用以训练中途对模型进行检验：每check_step个epoch进行一次测试和保存模型\n",
    "\n",
    "learning_rate = 1e-5  # 优化器的学习率\n",
    "\n",
    "# 获取训练、测试数据、分类类别总数\n",
    "train_data = processing.get_exist2021_data_temp(type='train', len=3437)\n",
    "test_data = processing.get_exist2021_data_temp(type='test', len=1000)\n",
    "categories = 2\n",
    "\n",
    "train_iter, test_iter = MyBERT.load_bert_data(train_data, test_data, batch_size)\n",
    "\n",
    "#固定写法，可以牢记，cuda代表Gpu\n",
    "# torch.cuda.is_available()可以查看当前Gpu是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练模型，因为这里是英文数据集，需要用在英文上的预训练模型：bert-base-uncased\n",
    "# uncased指该预训练模型对应的词表不区分字母的大小写\n",
    "# 详情可了解：https://huggingface.co/bert-base-uncased\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "# 创建模型 BertSST2Model\n",
    "model = MyBERT.MyBertModel(categories, pretrained_model_name)\n",
    "# 固定写法，将模型加载到device上，\n",
    "# 如果是GPU上运行，此时可以观察到GPU的显存增加\n",
    "model.to(device)\n",
    "\n",
    "# 训练过程\n",
    "# Adam是最近较为常用的优化器，详情可查看：https://www.jianshu.com/p/aebcaf8af76e\n",
    "optimizer = Adam(model.parameters(), learning_rate)  # 使用Adam优化器\n",
    "loss = nn.CrossEntropyLoss()  # 使用crossentropy作为二分类任务的损失函数\n",
    "\n",
    "# 记录当前训练时间，用以记录日志和存储\n",
    "timestamp = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|\u001b[31m██████████\u001b[0m| 215/215 [00:48<00:00,  4.41it/s]\n",
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 999/999 [00:11<00:00, 84.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[289 168]\n",
      " [ 97 445]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.63      0.69       457\n",
      "           1       0.73      0.82      0.77       542\n",
      "\n",
      "    accuracy                           0.73       999\n",
      "   macro avg       0.74      0.73      0.73       999\n",
      "weighted avg       0.74      0.73      0.73       999\n",
      "\n",
      "Acc : 0.7347347347347347\t F1: 0.7705627705627706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|\u001b[31m██████████\u001b[0m| 215/215 [00:47<00:00,  4.51it/s]\n",
      "Testing: 100%|\u001b[32m██████████\u001b[0m| 999/999 [00:11<00:00, 85.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[340 117]\n",
      " [147 395]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       457\n",
      "           1       0.77      0.73      0.75       542\n",
      "\n",
      "    accuracy                           0.74       999\n",
      "   macro avg       0.73      0.74      0.73       999\n",
      "weighted avg       0.74      0.74      0.74       999\n",
      "\n",
      "Acc : 0.7357357357357357\t F1: 0.7495256166982922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  33%|\u001b[31m███▎      \u001b[0m| 71/215 [00:15<00:32,  4.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Documents\\Experiment\\Ag\\EXIST_BERT\\bert.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/Experiment/Ag/EXIST_BERT/bert.ipynb#ch0000002?line=0'>1</a>\u001b[0m fp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(file_name, \u001b[39m'\u001b[39m\u001b[39ma+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/Experiment/Ag/EXIST_BERT/bert.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/Experiment/Ag/EXIST_BERT/bert.ipynb#ch0000002?line=2'>3</a>\u001b[0m     MyBERT\u001b[39m.\u001b[39;49mtrain(model, train_iter,device, optimizer, loss, epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Documents/Experiment/Ag/EXIST_BERT/bert.ipynb#ch0000002?line=3'>4</a>\u001b[0m     MyBERT\u001b[39m.\u001b[39mtest(model, test_iter, device, epoch, file_name)\n",
      "File \u001b[1;32m~\\Documents\\Experiment\\MyKu\\MyBERT.py:135\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, device, optimizer, loss, epoch)\u001b[0m\n\u001b[0;32m    133\u001b[0m l\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    134\u001b[0m \u001b[39m# 根据反向传播的值更新模型的参数\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    136\u001b[0m \u001b[39m# 统计总的损失，.item()方法用于取出tensor中的值\u001b[39;00m\n\u001b[0;32m    137\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    142\u001b[0m            grads,\n\u001b[0;32m    143\u001b[0m            exp_avgs,\n\u001b[0;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m            state_steps,\n\u001b[0;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Software\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m     94\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     96\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m     98\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fp = open(file_name, 'a+')\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    MyBERT.train(model, train_iter,device, optimizer, loss, epoch)\n",
    "    MyBERT.test(model, test_iter, device, epoch, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbee185f428494c0f3f4eb26ca27cba894eb4f3d4a4ff826385edccebf850a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
